{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def plot_confusion_matrix(y_test,y_pred,labels=None,lenth=20,ratio=False,cmap=\"Blues\",annot=True,width=12,fmt=\"g\"):\n",
    "    if labels == None:\n",
    "        labels = pd.Series(labels for labels in y_pred.squeeze()).unique()\n",
    "    conf_mat = pd.DataFrame(confusion_matrix(y_test.squeeze(), y_pred.squeeze(), labels=labels))\n",
    "    if ratio == True:\n",
    "        conf_mat = conf_mat.divide(conf_mat.sum(axis=1), axis=0)\n",
    "    fig, ax = plt.subplots(figsize=(lenth, width))\n",
    "    sns.heatmap(conf_mat,cmap=cmap,annot=annot,xticklabels=labels,yticklabels=labels,fmt=fmt)\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('word2idx_big.pkl','rb')\n",
    "word2idx = pickle.load(f)\n",
    "f.close()\n",
    "f = open('idx2word_big.pkl','rb')\n",
    "idx2word= pickle.load(f)\n",
    "f.close()\n",
    "f = open('df_big.pkl','rb')\n",
    "word_df= pickle.load(f)\n",
    "f.close()\n",
    "f = open('embedding_big.pkl','rb')\n",
    "word_embedding= pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_NUM = 0\n",
    "UNK_NUM = 1\n",
    "PADDLING_NUM = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    56821.000000\n",
       "mean         9.087344\n",
       "std          2.868494\n",
       "min          0.000000\n",
       "25%          7.000000\n",
       "50%          9.000000\n",
       "75%         11.000000\n",
       "max         48.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    56821.000000\n",
       "mean         9.087344\n",
       "std          2.868494\n",
       "min          0.000000\n",
       "25%          7.000000\n",
       "50%          9.000000\n",
       "75%         11.000000\n",
       "max         48.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df['text'].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355990"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordEmbedding(data):\n",
    "    seq_embed = []\n",
    "    _ = 0\n",
    "    for word in data:\n",
    "        seq_embed.append(word_embedding[word])\n",
    "        _ += 1\n",
    "        if _ == 9:\n",
    "            break\n",
    "    if _ <= 9:\n",
    "        for i in range(9 - _):\n",
    "            seq_embed.append(np.zeros(300))\n",
    "    seq_embed = np.array(seq_embed)\n",
    "    return seq_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getWordEmbedding(data):\n",
    "#     seq_embed = []\n",
    "#     _ = 0\n",
    "#     for word in data:\n",
    "#         seq_embed.append(word)\n",
    "#         _ += 1\n",
    "#         if _ == 9:\n",
    "#             break\n",
    "#     if _ <= 9:\n",
    "#         for i in range(9 - _):\n",
    "#             seq_embed.append()\n",
    "#     seq_embed = np.array(seq_embed)\n",
    "#     return seq_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df['text'] = word_df['text'].apply(getWordEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Javon\\AppData\\Local\\Temp/ipykernel_21220/72822735.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['class'] = train_df['class'].apply(np.float32)\n"
     ]
    }
   ],
   "source": [
    "train_df = word_df[['text','class']]\n",
    "train_df['class'] = train_df['class'].apply(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.77897102, -0.24356601, -0.26709101, ..., -0.371645  ,\n",
       "         1.04136801,  0.18475001],\n",
       "       [-0.005251  , -0.203603  , -0.764615  , ..., -0.31193799,\n",
       "         0.56219399, -0.002608  ],\n",
       "       [ 1.29011798, -0.39268801, -0.42361701, ...,  0.41950399,\n",
       "         0.20449001,  0.64937598],\n",
       "       ...,\n",
       "       [ 0.079874  , -0.130665  ,  0.27537099, ...,  0.214256  ,\n",
       "         0.33445001,  0.84352601],\n",
       "       [-0.29280099,  0.449341  ,  0.14328299, ...,  0.15883499,\n",
       "         0.67825699, -0.58762503],\n",
       "       [ 0.38094899,  0.39534199, -0.105029  , ...,  0.35614899,\n",
       "        -0.122519  ,  0.90290499]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    train_df['text'].values,\n",
    "    train_df['class'].values,\n",
    "    test_size=0.2,\n",
    "    stratify=train_df['class'],\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,label,transformer = None):\n",
    "        super(seqDataset,self).__init__()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transformer = transformer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if self.transformer:\n",
    "            return transformer(self.data[idx]),self.label[idx]\n",
    "        return self.data[idx],self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [np.array(i) for i in X_train]\n",
    "X_test = [np.array(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.533471, -0.738836, -0.745665, ...,  0.127822, -0.522074,\n",
       "         0.193523],\n",
       "       [-0.030646, -0.128678,  0.033818, ...,  0.174529, -0.073322,\n",
       "         0.034218],\n",
       "       [-0.365248, -0.336133, -0.045593, ...,  0.648002,  0.211448,\n",
       "         0.375935],\n",
       "       ...,\n",
       "       [-0.445826, -0.097827,  0.230186, ...,  0.051419, -0.328975,\n",
       "         0.01057 ],\n",
       "       [-0.10139 , -0.143253, -0.035549, ...,  0.140243, -0.162254,\n",
       "         0.017669],\n",
       "       [-0.060803,  0.300508,  0.512175, ..., -0.168457, -0.568124,\n",
       "         0.533321]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = seqDataset(X_train,y_train)\n",
    "test_dataset = seqDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=500)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 9, 300])\n"
     ]
    }
   ],
   "source": [
    "for i,(x,y) in enumerate(train_loader):\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,EPOCH=150,learning_rate=0.05):\n",
    "    model = model\n",
    "    opt = torch.optim.SGD(params=model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    EPOCH = EPOCH\n",
    "    itr = 0\n",
    "    model.cuda()\n",
    "    for epoch in range(EPOCH):\n",
    "        for i,(x,y) in enumerate(train_loader):\n",
    "            batch_X = torch.FloatTensor(np.float32(x)).cuda()\n",
    "            batch_Y = torch.LongTensor(np.int64(y)).cuda()\n",
    "            output = model(batch_X)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(output,batch_Y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if (i+1) % 20 == 0:\n",
    "                itr += 20\n",
    "                acc = (batch_Y.detach().cpu().numpy() == output.detach().cpu().argmax(axis=1).numpy()).mean()\n",
    "                print('epoch\\t:{}\\titr:{}\\tloss:{}\\tacc:{}\\n'.format(epoch,itr,loss,acc))\n",
    "                # print(output.detach().cpu())\n",
    "    return model\n",
    "\n",
    "def test(model):\n",
    "    accs = []\n",
    "    for i,(x,y) in enumerate(test_loader):\n",
    "        batch_X = torch.FloatTensor(np.float32(x)).cuda()\n",
    "        batch_Y = torch.LongTensor(np.int64(y)).cuda()\n",
    "        output = model(batch_X)\n",
    "        acc = (batch_Y.detach().cpu().numpy() == output.detach().cpu().argmax(axis=1).numpy()).mean()\n",
    "        accs.append(acc)\n",
    "    print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,seq_len=9,embedding_size=300,h1_size=128,num_embeddings = len(idx2word)):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.h1_size = h1_size\n",
    "        self.seq_len = seq_len\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # self.embed = nn.Embedding(num_embeddings=self.num_embeddings,embedding_dim=self.embedding_size)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.RNN(input_size = self.embedding_size,hidden_size = self.h1_size,num_layers=4),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.seq_len*self.h1_size,10),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # out = self.embed(x)\n",
    "        out,hid = self.layer1(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,seq_len=9,embedding_size=300,num_embeddings = len(idx2word)):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.num_embeddings = num_embeddings \n",
    "        self.embedding_size = embedding_size\n",
    "        self.seq_len = seq_len\n",
    "        self.emb = nn.Embedding(num_embeddings=self.num_embeddings,embedding_dim=self.embedding_size)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_size, nhead=10, batch_first=True),\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_size, nhead=10, batch_first=True),\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_size, nhead=10, batch_first=True),\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_size, nhead=10, batch_first=True),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.seq_len*self.embedding_size,10),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # out = self.emb(x)\n",
    "        out = self.layer1(x)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self,embedding_size=300,num_embeddings = len(idx2word)):\n",
    "        super(SimpleCNN,self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_size = embedding_size\n",
    "        # self.embed = nn.Embedding(num_embeddings=self.num_embeddings,embedding_dim=self.embedding_size)\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 300,out_channels = 300,kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(300),\n",
    "        )\n",
    "        #[batch_size = 500,channels=128,seq_len - k_s + 1 = 8]\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 300,out_channels = 200,kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(200),\n",
    "        )\n",
    "        #[batch_size = 500,embedding_size = 128,seq_len - k_s + 1 = 7]\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 200,out_channels = 100,kernel_size=3,stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            # nn.MaxPool1d(6),\n",
    "        )\n",
    "        #[batch_size = 500,embedding_size = 256,(seq_len - k_s)/2+ 1 = 3]\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 100,out_channels = 100,kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "        )\n",
    "        #[batch_size = 500,embedding_size = 256,seq_len - k_s + 1 = 1]\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(100,10),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        # out = self.embed(x).transpose(2,1)\n",
    "        out = self.conv1(x.transpose(2,1))\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t:0\titr:20\tloss:2.119532585144043\tacc:0.374\n",
      "\n",
      "epoch\t:0\titr:40\tloss:2.013864040374756\tacc:0.468\n",
      "\n",
      "epoch\t:0\titr:60\tloss:1.9531245231628418\tacc:0.544\n",
      "\n",
      "epoch\t:0\titr:80\tloss:1.8542976379394531\tacc:0.654\n",
      "\n",
      "epoch\t:1\titr:100\tloss:1.8420692682266235\tacc:0.648\n",
      "\n",
      "epoch\t:1\titr:120\tloss:1.838194727897644\tacc:0.648\n",
      "\n",
      "epoch\t:1\titr:140\tloss:1.832452654838562\tacc:0.648\n",
      "\n",
      "epoch\t:1\titr:160\tloss:1.7865103483200073\tacc:0.694\n",
      "\n",
      "epoch\t:2\titr:180\tloss:1.7969391345977783\tacc:0.674\n",
      "\n",
      "epoch\t:2\titr:200\tloss:1.783646821975708\tacc:0.702\n",
      "\n",
      "epoch\t:2\titr:220\tloss:1.7917819023132324\tacc:0.674\n",
      "\n",
      "epoch\t:2\titr:240\tloss:1.7515790462493896\tacc:0.728\n",
      "\n",
      "epoch\t:3\titr:260\tloss:1.7646020650863647\tacc:0.708\n",
      "\n",
      "epoch\t:3\titr:280\tloss:1.7493075132369995\tacc:0.724\n",
      "\n",
      "epoch\t:3\titr:300\tloss:1.7680319547653198\tacc:0.694\n",
      "\n",
      "epoch\t:3\titr:320\tloss:1.7253509759902954\tacc:0.754\n",
      "\n",
      "epoch\t:4\titr:340\tloss:1.7451316118240356\tacc:0.726\n",
      "\n",
      "epoch\t:4\titr:360\tloss:1.7425172328948975\tacc:0.72\n",
      "\n",
      "epoch\t:4\titr:380\tloss:1.7508376836776733\tacc:0.72\n",
      "\n",
      "epoch\t:4\titr:400\tloss:1.7158458232879639\tacc:0.752\n",
      "\n",
      "epoch\t:5\titr:420\tloss:1.7370814085006714\tacc:0.736\n",
      "\n",
      "epoch\t:5\titr:440\tloss:1.7317919731140137\tacc:0.74\n",
      "\n",
      "epoch\t:5\titr:460\tloss:1.7367157936096191\tacc:0.74\n",
      "\n",
      "epoch\t:5\titr:480\tloss:1.706936001777649\tacc:0.766\n",
      "\n",
      "epoch\t:6\titr:500\tloss:1.7260469198226929\tacc:0.746\n",
      "\n",
      "epoch\t:6\titr:520\tloss:1.7238428592681885\tacc:0.756\n",
      "\n",
      "epoch\t:6\titr:540\tloss:1.7324289083480835\tacc:0.74\n",
      "\n",
      "epoch\t:6\titr:560\tloss:1.7009369134902954\tacc:0.766\n",
      "\n",
      "epoch\t:7\titr:580\tloss:1.7227764129638672\tacc:0.748\n",
      "\n",
      "epoch\t:7\titr:600\tloss:1.7182590961456299\tacc:0.752\n",
      "\n",
      "epoch\t:7\titr:620\tloss:1.7227635383605957\tacc:0.746\n",
      "\n",
      "epoch\t:7\titr:640\tloss:1.6934735774993896\tacc:0.778\n",
      "\n",
      "epoch\t:8\titr:660\tloss:1.7130675315856934\tacc:0.762\n",
      "\n",
      "epoch\t:8\titr:680\tloss:1.7147763967514038\tacc:0.754\n",
      "\n",
      "epoch\t:8\titr:700\tloss:1.7181459665298462\tacc:0.75\n",
      "\n",
      "epoch\t:8\titr:720\tloss:1.6934772729873657\tacc:0.776\n",
      "\n",
      "epoch\t:9\titr:740\tloss:1.7113350629806519\tacc:0.768\n",
      "\n",
      "epoch\t:9\titr:760\tloss:1.7097547054290771\tacc:0.754\n",
      "\n",
      "epoch\t:9\titr:780\tloss:1.7072809934616089\tacc:0.772\n",
      "\n",
      "epoch\t:9\titr:800\tloss:1.6887249946594238\tacc:0.788\n",
      "\n",
      "epoch\t:10\titr:820\tloss:1.7110905647277832\tacc:0.754\n",
      "\n",
      "epoch\t:10\titr:840\tloss:1.7046146392822266\tacc:0.76\n",
      "\n",
      "epoch\t:10\titr:860\tloss:1.7081823348999023\tacc:0.768\n",
      "\n",
      "epoch\t:10\titr:880\tloss:1.690337896347046\tacc:0.78\n",
      "\n",
      "epoch\t:11\titr:900\tloss:1.7105540037155151\tacc:0.762\n",
      "\n",
      "epoch\t:11\titr:920\tloss:1.6999735832214355\tacc:0.77\n",
      "\n",
      "epoch\t:11\titr:940\tloss:1.7056902647018433\tacc:0.754\n",
      "\n",
      "epoch\t:11\titr:960\tloss:1.6922001838684082\tacc:0.78\n",
      "\n",
      "epoch\t:12\titr:980\tloss:1.7066479921340942\tacc:0.764\n",
      "\n",
      "epoch\t:12\titr:1000\tloss:1.6959567070007324\tacc:0.774\n",
      "\n",
      "epoch\t:12\titr:1020\tloss:1.7003015279769897\tacc:0.774\n",
      "\n",
      "epoch\t:12\titr:1040\tloss:1.6843706369400024\tacc:0.782\n",
      "\n",
      "epoch\t:13\titr:1060\tloss:1.700554370880127\tacc:0.764\n",
      "\n",
      "epoch\t:13\titr:1080\tloss:1.6952815055847168\tacc:0.774\n",
      "\n",
      "epoch\t:13\titr:1100\tloss:1.7052297592163086\tacc:0.752\n",
      "\n",
      "epoch\t:13\titr:1120\tloss:1.6802160739898682\tacc:0.79\n",
      "\n",
      "epoch\t:14\titr:1140\tloss:1.7022117376327515\tacc:0.764\n",
      "\n",
      "epoch\t:14\titr:1160\tloss:1.6914695501327515\tacc:0.778\n",
      "\n",
      "epoch\t:14\titr:1180\tloss:1.6939812898635864\tacc:0.776\n",
      "\n",
      "epoch\t:14\titr:1200\tloss:1.6816158294677734\tacc:0.786\n",
      "\n",
      "epoch\t:15\titr:1220\tloss:1.6995388269424438\tacc:0.772\n",
      "\n",
      "epoch\t:15\titr:1240\tloss:1.6908947229385376\tacc:0.776\n",
      "\n",
      "epoch\t:15\titr:1260\tloss:1.6928929090499878\tacc:0.772\n",
      "\n",
      "epoch\t:15\titr:1280\tloss:1.6727865934371948\tacc:0.8\n",
      "\n",
      "epoch\t:16\titr:1300\tloss:1.6988434791564941\tacc:0.77\n",
      "\n",
      "epoch\t:16\titr:1320\tloss:1.6896165609359741\tacc:0.78\n",
      "\n",
      "epoch\t:16\titr:1340\tloss:1.6929538249969482\tacc:0.776\n",
      "\n",
      "epoch\t:16\titr:1360\tloss:1.6821988821029663\tacc:0.788\n",
      "\n",
      "epoch\t:17\titr:1380\tloss:1.6949598789215088\tacc:0.78\n",
      "\n",
      "epoch\t:17\titr:1400\tloss:1.6891980171203613\tacc:0.78\n",
      "\n",
      "epoch\t:17\titr:1420\tloss:1.6883883476257324\tacc:0.782\n",
      "\n",
      "epoch\t:17\titr:1440\tloss:1.6796810626983643\tacc:0.79\n",
      "\n",
      "epoch\t:18\titr:1460\tloss:1.6906161308288574\tacc:0.782\n",
      "\n",
      "epoch\t:18\titr:1480\tloss:1.6861252784729004\tacc:0.782\n",
      "\n",
      "epoch\t:18\titr:1500\tloss:1.6869957447052002\tacc:0.782\n",
      "\n",
      "epoch\t:18\titr:1520\tloss:1.6700743436813354\tacc:0.802\n",
      "\n",
      "epoch\t:19\titr:1540\tloss:1.6921316385269165\tacc:0.782\n",
      "\n",
      "epoch\t:19\titr:1560\tloss:1.6784281730651855\tacc:0.794\n",
      "\n",
      "epoch\t:19\titr:1580\tloss:1.6872817277908325\tacc:0.784\n",
      "\n",
      "epoch\t:19\titr:1600\tloss:1.6689701080322266\tacc:0.796\n",
      "\n",
      "epoch\t:20\titr:1620\tloss:1.692250370979309\tacc:0.78\n",
      "\n",
      "epoch\t:20\titr:1640\tloss:1.6810678243637085\tacc:0.788\n",
      "\n",
      "epoch\t:20\titr:1660\tloss:1.6831756830215454\tacc:0.786\n",
      "\n",
      "epoch\t:20\titr:1680\tloss:1.669003963470459\tacc:0.804\n",
      "\n",
      "epoch\t:21\titr:1700\tloss:1.692671537399292\tacc:0.778\n",
      "\n",
      "epoch\t:21\titr:1720\tloss:1.6875044107437134\tacc:0.772\n",
      "\n",
      "epoch\t:21\titr:1740\tloss:1.6826194524765015\tacc:0.79\n",
      "\n",
      "epoch\t:21\titr:1760\tloss:1.6651065349578857\tacc:0.808\n",
      "\n",
      "epoch\t:22\titr:1780\tloss:1.6843035221099854\tacc:0.784\n",
      "\n",
      "epoch\t:22\titr:1800\tloss:1.6790382862091064\tacc:0.79\n",
      "\n",
      "epoch\t:22\titr:1820\tloss:1.6788585186004639\tacc:0.792\n",
      "\n",
      "epoch\t:22\titr:1840\tloss:1.6645805835723877\tacc:0.804\n",
      "\n",
      "epoch\t:23\titr:1860\tloss:1.687422275543213\tacc:0.782\n",
      "\n",
      "epoch\t:23\titr:1880\tloss:1.6739774942398071\tacc:0.796\n",
      "\n",
      "epoch\t:23\titr:1900\tloss:1.6831084489822388\tacc:0.786\n",
      "\n",
      "epoch\t:23\titr:1920\tloss:1.664438009262085\tacc:0.802\n",
      "\n",
      "epoch\t:24\titr:1940\tloss:1.684698224067688\tacc:0.78\n",
      "\n",
      "epoch\t:24\titr:1960\tloss:1.67729914188385\tacc:0.792\n",
      "\n",
      "epoch\t:24\titr:1980\tloss:1.6751781702041626\tacc:0.8\n",
      "\n",
      "epoch\t:24\titr:2000\tloss:1.659852147102356\tacc:0.81\n",
      "\n",
      "epoch\t:25\titr:2020\tloss:1.6854661703109741\tacc:0.782\n",
      "\n",
      "epoch\t:25\titr:2040\tloss:1.6796517372131348\tacc:0.79\n",
      "\n",
      "epoch\t:25\titr:2060\tloss:1.6726025342941284\tacc:0.798\n",
      "\n",
      "epoch\t:25\titr:2080\tloss:1.6603116989135742\tacc:0.81\n",
      "\n",
      "epoch\t:26\titr:2100\tloss:1.6833465099334717\tacc:0.782\n",
      "\n",
      "epoch\t:26\titr:2120\tloss:1.6733640432357788\tacc:0.796\n",
      "\n",
      "epoch\t:26\titr:2140\tloss:1.6705049276351929\tacc:0.794\n",
      "\n",
      "epoch\t:26\titr:2160\tloss:1.6567330360412598\tacc:0.816\n",
      "\n",
      "epoch\t:27\titr:2180\tloss:1.6786495447158813\tacc:0.796\n",
      "\n",
      "epoch\t:27\titr:2200\tloss:1.6783158779144287\tacc:0.784\n",
      "\n",
      "epoch\t:27\titr:2220\tloss:1.666136622428894\tacc:0.804\n",
      "\n",
      "epoch\t:27\titr:2240\tloss:1.6576294898986816\tacc:0.816\n",
      "\n",
      "epoch\t:28\titr:2260\tloss:1.6819859743118286\tacc:0.792\n",
      "\n",
      "epoch\t:28\titr:2280\tloss:1.6714019775390625\tacc:0.796\n",
      "\n",
      "epoch\t:28\titr:2300\tloss:1.6636544466018677\tacc:0.81\n",
      "\n",
      "epoch\t:28\titr:2320\tloss:1.6547000408172607\tacc:0.814\n",
      "\n",
      "epoch\t:29\titr:2340\tloss:1.67259681224823\tacc:0.802\n",
      "\n",
      "epoch\t:29\titr:2360\tloss:1.671489953994751\tacc:0.796\n",
      "\n",
      "epoch\t:29\titr:2380\tloss:1.6668851375579834\tacc:0.806\n",
      "\n",
      "epoch\t:29\titr:2400\tloss:1.6587351560592651\tacc:0.812\n",
      "\n",
      "epoch\t:30\titr:2420\tloss:1.6755112409591675\tacc:0.8\n",
      "\n",
      "epoch\t:30\titr:2440\tloss:1.664940595626831\tacc:0.802\n",
      "\n",
      "epoch\t:30\titr:2460\tloss:1.6650291681289673\tacc:0.808\n",
      "\n",
      "epoch\t:30\titr:2480\tloss:1.659580945968628\tacc:0.814\n",
      "\n",
      "epoch\t:31\titr:2500\tloss:1.6750657558441162\tacc:0.796\n",
      "\n",
      "epoch\t:31\titr:2520\tloss:1.6631938219070435\tacc:0.808\n",
      "\n",
      "epoch\t:31\titr:2540\tloss:1.6640722751617432\tacc:0.81\n",
      "\n",
      "epoch\t:31\titr:2560\tloss:1.657006025314331\tacc:0.814\n",
      "\n",
      "epoch\t:32\titr:2580\tloss:1.6828938722610474\tacc:0.788\n",
      "\n",
      "epoch\t:32\titr:2600\tloss:1.6620122194290161\tacc:0.806\n",
      "\n",
      "epoch\t:32\titr:2620\tloss:1.6620086431503296\tacc:0.806\n",
      "\n",
      "epoch\t:32\titr:2640\tloss:1.6535357236862183\tacc:0.814\n",
      "\n",
      "epoch\t:33\titr:2660\tloss:1.6676990985870361\tacc:0.804\n",
      "\n",
      "epoch\t:33\titr:2680\tloss:1.6665271520614624\tacc:0.802\n",
      "\n",
      "epoch\t:33\titr:2700\tloss:1.660628080368042\tacc:0.812\n",
      "\n",
      "epoch\t:33\titr:2720\tloss:1.6582164764404297\tacc:0.816\n",
      "\n",
      "epoch\t:34\titr:2740\tloss:1.6779773235321045\tacc:0.788\n",
      "\n",
      "epoch\t:34\titr:2760\tloss:1.658390760421753\tacc:0.814\n",
      "\n",
      "epoch\t:34\titr:2780\tloss:1.6561404466629028\tacc:0.814\n",
      "\n",
      "epoch\t:34\titr:2800\tloss:1.650535225868225\tacc:0.82\n",
      "\n",
      "epoch\t:35\titr:2820\tloss:1.6689401865005493\tacc:0.802\n",
      "\n",
      "epoch\t:35\titr:2840\tloss:1.6565431356430054\tacc:0.81\n",
      "\n",
      "epoch\t:35\titr:2860\tloss:1.652664303779602\tacc:0.82\n",
      "\n",
      "epoch\t:35\titr:2880\tloss:1.6500375270843506\tacc:0.822\n",
      "\n",
      "epoch\t:36\titr:2900\tloss:1.676041841506958\tacc:0.796\n",
      "\n",
      "epoch\t:36\titr:2920\tloss:1.657419204711914\tacc:0.814\n",
      "\n",
      "epoch\t:36\titr:2940\tloss:1.6614717245101929\tacc:0.808\n",
      "\n",
      "epoch\t:36\titr:2960\tloss:1.651492714881897\tacc:0.814\n",
      "\n",
      "epoch\t:37\titr:2980\tloss:1.6683293581008911\tacc:0.806\n",
      "\n",
      "epoch\t:37\titr:3000\tloss:1.650864839553833\tacc:0.818\n",
      "\n",
      "epoch\t:37\titr:3020\tloss:1.6534391641616821\tacc:0.818\n",
      "\n",
      "epoch\t:37\titr:3040\tloss:1.660338282585144\tacc:0.81\n",
      "\n",
      "epoch\t:38\titr:3060\tloss:1.6624813079833984\tacc:0.802\n",
      "\n",
      "epoch\t:38\titr:3080\tloss:1.6646217107772827\tacc:0.804\n",
      "\n",
      "epoch\t:38\titr:3100\tloss:1.6531671285629272\tacc:0.814\n",
      "\n",
      "epoch\t:38\titr:3120\tloss:1.6499096155166626\tacc:0.822\n",
      "\n",
      "epoch\t:39\titr:3140\tloss:1.6648908853530884\tacc:0.804\n",
      "\n",
      "epoch\t:39\titr:3160\tloss:1.6598173379898071\tacc:0.806\n",
      "\n",
      "epoch\t:39\titr:3180\tloss:1.6526817083358765\tacc:0.818\n",
      "\n",
      "epoch\t:39\titr:3200\tloss:1.6492036581039429\tacc:0.822\n",
      "\n",
      "epoch\t:40\titr:3220\tloss:1.6681668758392334\tacc:0.8\n",
      "\n",
      "epoch\t:40\titr:3240\tloss:1.6566683053970337\tacc:0.814\n",
      "\n",
      "epoch\t:40\titr:3260\tloss:1.6480889320373535\tacc:0.824\n",
      "\n",
      "epoch\t:40\titr:3280\tloss:1.6502141952514648\tacc:0.82\n",
      "\n",
      "epoch\t:41\titr:3300\tloss:1.6666302680969238\tacc:0.796\n",
      "\n",
      "epoch\t:41\titr:3320\tloss:1.6557278633117676\tacc:0.816\n",
      "\n",
      "epoch\t:41\titr:3340\tloss:1.6498420238494873\tacc:0.82\n",
      "\n",
      "epoch\t:41\titr:3360\tloss:1.6430559158325195\tacc:0.824\n",
      "\n",
      "epoch\t:42\titr:3380\tloss:1.6606559753417969\tacc:0.81\n",
      "\n",
      "epoch\t:42\titr:3400\tloss:1.6520308256149292\tacc:0.816\n",
      "\n",
      "epoch\t:42\titr:3420\tloss:1.6487245559692383\tacc:0.824\n",
      "\n",
      "epoch\t:42\titr:3440\tloss:1.6424404382705688\tacc:0.822\n",
      "\n",
      "epoch\t:43\titr:3460\tloss:1.6616860628128052\tacc:0.81\n",
      "\n",
      "epoch\t:43\titr:3480\tloss:1.651231050491333\tacc:0.82\n",
      "\n",
      "epoch\t:43\titr:3500\tloss:1.6457557678222656\tacc:0.824\n",
      "\n",
      "epoch\t:43\titr:3520\tloss:1.6446772813796997\tacc:0.824\n",
      "\n",
      "epoch\t:44\titr:3540\tloss:1.6568481922149658\tacc:0.812\n",
      "\n",
      "epoch\t:44\titr:3560\tloss:1.6565226316452026\tacc:0.814\n",
      "\n",
      "epoch\t:44\titr:3580\tloss:1.647325038909912\tacc:0.82\n",
      "\n",
      "epoch\t:44\titr:3600\tloss:1.6482133865356445\tacc:0.818\n",
      "\n",
      "epoch\t:45\titr:3620\tloss:1.6624811887741089\tacc:0.806\n",
      "\n",
      "epoch\t:45\titr:3640\tloss:1.651662826538086\tacc:0.812\n",
      "\n",
      "epoch\t:45\titr:3660\tloss:1.6432832479476929\tacc:0.826\n",
      "\n",
      "epoch\t:45\titr:3680\tloss:1.6423850059509277\tacc:0.822\n",
      "\n",
      "epoch\t:46\titr:3700\tloss:1.6597720384597778\tacc:0.806\n",
      "\n",
      "epoch\t:46\titr:3720\tloss:1.6497387886047363\tacc:0.818\n",
      "\n",
      "epoch\t:46\titr:3740\tloss:1.6507984399795532\tacc:0.818\n",
      "\n",
      "epoch\t:46\titr:3760\tloss:1.6418455839157104\tacc:0.824\n",
      "\n",
      "epoch\t:47\titr:3780\tloss:1.6576586961746216\tacc:0.808\n",
      "\n",
      "epoch\t:47\titr:3800\tloss:1.6499650478363037\tacc:0.814\n",
      "\n",
      "epoch\t:47\titr:3820\tloss:1.640902042388916\tacc:0.828\n",
      "\n",
      "epoch\t:47\titr:3840\tloss:1.6413427591323853\tacc:0.822\n",
      "\n",
      "epoch\t:48\titr:3860\tloss:1.6598373651504517\tacc:0.81\n",
      "\n",
      "epoch\t:48\titr:3880\tloss:1.6469873189926147\tacc:0.822\n",
      "\n",
      "epoch\t:48\titr:3900\tloss:1.6395392417907715\tacc:0.832\n",
      "\n",
      "epoch\t:48\titr:3920\tloss:1.6458078622817993\tacc:0.822\n",
      "\n",
      "epoch\t:49\titr:3940\tloss:1.651887059211731\tacc:0.818\n",
      "\n",
      "epoch\t:49\titr:3960\tloss:1.642521619796753\tacc:0.824\n",
      "\n",
      "epoch\t:49\titr:3980\tloss:1.649989128112793\tacc:0.816\n",
      "\n",
      "epoch\t:49\titr:4000\tloss:1.6402989625930786\tacc:0.828\n",
      "\n",
      "epoch\t:50\titr:4020\tloss:1.6577869653701782\tacc:0.81\n",
      "\n",
      "epoch\t:50\titr:4040\tloss:1.6445070505142212\tacc:0.826\n",
      "\n",
      "epoch\t:50\titr:4060\tloss:1.6482086181640625\tacc:0.82\n",
      "\n",
      "epoch\t:50\titr:4080\tloss:1.6424986124038696\tacc:0.824\n",
      "\n",
      "epoch\t:51\titr:4100\tloss:1.659623146057129\tacc:0.814\n",
      "\n",
      "epoch\t:51\titr:4120\tloss:1.6434245109558105\tacc:0.824\n",
      "\n",
      "epoch\t:51\titr:4140\tloss:1.637372612953186\tacc:0.832\n",
      "\n",
      "epoch\t:51\titr:4160\tloss:1.637740135192871\tacc:0.828\n",
      "\n",
      "epoch\t:52\titr:4180\tloss:1.6504015922546387\tacc:0.814\n",
      "\n",
      "epoch\t:52\titr:4200\tloss:1.6404770612716675\tacc:0.83\n",
      "\n",
      "epoch\t:52\titr:4220\tloss:1.6377733945846558\tacc:0.828\n",
      "\n",
      "epoch\t:52\titr:4240\tloss:1.6398056745529175\tacc:0.826\n",
      "\n",
      "epoch\t:53\titr:4260\tloss:1.652245283126831\tacc:0.82\n",
      "\n",
      "epoch\t:53\titr:4280\tloss:1.644808292388916\tacc:0.824\n",
      "\n",
      "epoch\t:53\titr:4300\tloss:1.6356853246688843\tacc:0.83\n",
      "\n",
      "epoch\t:53\titr:4320\tloss:1.6374452114105225\tacc:0.83\n",
      "\n",
      "epoch\t:54\titr:4340\tloss:1.6516244411468506\tacc:0.816\n",
      "\n",
      "epoch\t:54\titr:4360\tloss:1.6378847360610962\tacc:0.83\n",
      "\n",
      "epoch\t:54\titr:4380\tloss:1.6366647481918335\tacc:0.834\n",
      "\n",
      "epoch\t:54\titr:4400\tloss:1.638113021850586\tacc:0.826\n",
      "\n",
      "epoch\t:55\titr:4420\tloss:1.655830979347229\tacc:0.812\n",
      "\n",
      "epoch\t:55\titr:4440\tloss:1.6397126913070679\tacc:0.826\n",
      "\n",
      "epoch\t:55\titr:4460\tloss:1.6381605863571167\tacc:0.826\n",
      "\n",
      "epoch\t:55\titr:4480\tloss:1.6423248052597046\tacc:0.826\n",
      "\n",
      "epoch\t:56\titr:4500\tloss:1.6585206985473633\tacc:0.81\n",
      "\n",
      "epoch\t:56\titr:4520\tloss:1.6337926387786865\tacc:0.832\n",
      "\n",
      "epoch\t:56\titr:4540\tloss:1.6382519006729126\tacc:0.83\n",
      "\n",
      "epoch\t:56\titr:4560\tloss:1.6330928802490234\tacc:0.832\n",
      "\n",
      "epoch\t:57\titr:4580\tloss:1.6586860418319702\tacc:0.81\n",
      "\n",
      "epoch\t:57\titr:4600\tloss:1.6364601850509644\tacc:0.83\n",
      "\n",
      "epoch\t:57\titr:4620\tloss:1.6327744722366333\tacc:0.834\n",
      "\n",
      "epoch\t:57\titr:4640\tloss:1.639840841293335\tacc:0.828\n",
      "\n",
      "epoch\t:58\titr:4660\tloss:1.6465786695480347\tacc:0.82\n",
      "\n",
      "epoch\t:58\titr:4680\tloss:1.6382672786712646\tacc:0.832\n",
      "\n",
      "epoch\t:58\titr:4700\tloss:1.6308571100234985\tacc:0.834\n",
      "\n",
      "epoch\t:58\titr:4720\tloss:1.6310421228408813\tacc:0.834\n",
      "\n",
      "epoch\t:59\titr:4740\tloss:1.6470485925674438\tacc:0.822\n",
      "\n",
      "epoch\t:59\titr:4760\tloss:1.6366766691207886\tacc:0.834\n",
      "\n",
      "epoch\t:59\titr:4780\tloss:1.6370807886123657\tacc:0.828\n",
      "\n",
      "epoch\t:59\titr:4800\tloss:1.6320433616638184\tacc:0.834\n",
      "\n",
      "epoch\t:60\titr:4820\tloss:1.6500409841537476\tacc:0.816\n",
      "\n",
      "epoch\t:60\titr:4840\tloss:1.637358546257019\tacc:0.83\n",
      "\n",
      "epoch\t:60\titr:4860\tloss:1.628825068473816\tacc:0.838\n",
      "\n",
      "epoch\t:60\titr:4880\tloss:1.6289286613464355\tacc:0.836\n",
      "\n",
      "epoch\t:61\titr:4900\tloss:1.6475954055786133\tacc:0.824\n",
      "\n",
      "epoch\t:61\titr:4920\tloss:1.6287357807159424\tacc:0.838\n",
      "\n",
      "epoch\t:61\titr:4940\tloss:1.627431869506836\tacc:0.84\n",
      "\n",
      "epoch\t:61\titr:4960\tloss:1.6294500827789307\tacc:0.838\n",
      "\n",
      "epoch\t:62\titr:4980\tloss:1.643594741821289\tacc:0.826\n",
      "\n",
      "epoch\t:62\titr:5000\tloss:1.6355525255203247\tacc:0.83\n",
      "\n",
      "epoch\t:62\titr:5020\tloss:1.6255974769592285\tacc:0.842\n",
      "\n",
      "epoch\t:62\titr:5040\tloss:1.632279634475708\tacc:0.832\n",
      "\n",
      "epoch\t:63\titr:5060\tloss:1.6469839811325073\tacc:0.822\n",
      "\n",
      "epoch\t:63\titr:5080\tloss:1.6383450031280518\tacc:0.828\n",
      "\n",
      "epoch\t:63\titr:5100\tloss:1.6262609958648682\tacc:0.838\n",
      "\n",
      "epoch\t:63\titr:5120\tloss:1.6283252239227295\tacc:0.838\n",
      "\n",
      "epoch\t:64\titr:5140\tloss:1.6406817436218262\tacc:0.826\n",
      "\n",
      "epoch\t:64\titr:5160\tloss:1.6334264278411865\tacc:0.832\n",
      "\n",
      "epoch\t:64\titr:5180\tloss:1.625673532485962\tacc:0.846\n",
      "\n",
      "epoch\t:64\titr:5200\tloss:1.6261168718338013\tacc:0.84\n",
      "\n",
      "epoch\t:65\titr:5220\tloss:1.6442978382110596\tacc:0.826\n",
      "\n",
      "epoch\t:65\titr:5240\tloss:1.6332935094833374\tacc:0.834\n",
      "\n",
      "epoch\t:65\titr:5260\tloss:1.625960111618042\tacc:0.84\n",
      "\n",
      "epoch\t:65\titr:5280\tloss:1.6249898672103882\tacc:0.842\n",
      "\n",
      "epoch\t:66\titr:5300\tloss:1.6454704999923706\tacc:0.822\n",
      "\n",
      "epoch\t:66\titr:5320\tloss:1.6287837028503418\tacc:0.838\n",
      "\n",
      "epoch\t:66\titr:5340\tloss:1.6289589405059814\tacc:0.84\n",
      "\n",
      "epoch\t:66\titr:5360\tloss:1.6261775493621826\tacc:0.84\n",
      "\n",
      "epoch\t:67\titr:5380\tloss:1.6461753845214844\tacc:0.824\n",
      "\n",
      "epoch\t:67\titr:5400\tloss:1.628659963607788\tacc:0.836\n",
      "\n",
      "epoch\t:67\titr:5420\tloss:1.6232229471206665\tacc:0.842\n",
      "\n",
      "epoch\t:67\titr:5440\tloss:1.6257997751235962\tacc:0.842\n",
      "\n",
      "epoch\t:68\titr:5460\tloss:1.6414239406585693\tacc:0.828\n",
      "\n",
      "epoch\t:68\titr:5480\tloss:1.6306606531143188\tacc:0.838\n",
      "\n",
      "epoch\t:68\titr:5500\tloss:1.6221907138824463\tacc:0.844\n",
      "\n",
      "epoch\t:68\titr:5520\tloss:1.6249783039093018\tacc:0.844\n",
      "\n",
      "epoch\t:69\titr:5540\tloss:1.643011450767517\tacc:0.824\n",
      "\n",
      "epoch\t:69\titr:5560\tloss:1.6287912130355835\tacc:0.834\n",
      "\n",
      "epoch\t:69\titr:5580\tloss:1.6265637874603271\tacc:0.838\n",
      "\n",
      "epoch\t:69\titr:5600\tloss:1.6239625215530396\tacc:0.84\n",
      "\n",
      "epoch\t:70\titr:5620\tloss:1.6442036628723145\tacc:0.82\n",
      "\n",
      "epoch\t:70\titr:5640\tloss:1.6248526573181152\tacc:0.842\n",
      "\n",
      "epoch\t:70\titr:5660\tloss:1.6244693994522095\tacc:0.842\n",
      "\n",
      "epoch\t:70\titr:5680\tloss:1.6219686269760132\tacc:0.846\n",
      "\n",
      "epoch\t:71\titr:5700\tloss:1.643930435180664\tacc:0.822\n",
      "\n",
      "epoch\t:71\titr:5720\tloss:1.6312980651855469\tacc:0.836\n",
      "\n",
      "epoch\t:71\titr:5740\tloss:1.6199032068252563\tacc:0.846\n",
      "\n",
      "epoch\t:71\titr:5760\tloss:1.6247355937957764\tacc:0.844\n",
      "\n",
      "epoch\t:72\titr:5780\tloss:1.6418113708496094\tacc:0.826\n",
      "\n",
      "epoch\t:72\titr:5800\tloss:1.6277949810028076\tacc:0.84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = train(TransformerEncoder())\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(SimpleCNN())\n",
    "test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(SimpleRNN())\n",
    "test(model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd51592dbbd9f65b270c81363871e02974afbff07f8adfbdf2e97d0878249623"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
